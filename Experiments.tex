
\chapter{Experiments}


\paragraph{}
In this chapter we take an overview of the experiments that we used to evaluate our system. We analyze the results, and present some problems of our approach. Finally we compare our system with some state-of-the-art solutions.


\section{Datasets}
The system was trained and optimized using the datasets from the Making Sense of Microposts challenge~\cite{rizzo2015making}. The task of the challenge is to automatically recognize entities and their type from English tweets, and link them to the corresponding DBpedia resource.  

We used the datasets from the 2015 and 2016 challenge. These datasets are extracted from a subsets of tweets published between 2011 and 2015 selected using hashtags. These tweets are of two types, events tweet which are more likely to contain entities, and non-event tweets, these are more likely to not contain any entity. 
Another micropost challenge on entity linking is NEEL-IT organized by EVALITA~\cite{basile2016evalita}, they focus on the development of NEEL system for Italian tweets. In this case the dataset available was from the 2016 challenge. 


These type of challenge provide two or three datasets, the first is called ``training set" and will be used for training the models, an optional one called ``dev set" used for fine tuning of the algorithms and self evaluation of the system, and the last one is the ``test set", this will be used for the competition as official results for the system performance. Datasets also come with an annotated version of the tweets that contains the correct entities and links. This is called the ground truth or gold standard, and is used to evaluate the performance of the system, and allows researchers to perform changes on their system. The ground truth of the ``test set" is released after the submission of the participants. 

In Table \ref{tab:datasets} we can see each dataset size for both English and Italian micropost challenges. The table contains the total number of entities, the number of linkable entities, and the number of NIL entities.

\begin{table}[!htbp]
\centering
\footnotesize
\setlength{\tabcolsep}{0.3em}
\begin{tabularx}{\linewidth}{CCCCCCC}
\midrule
 & \multicolumn{3}{c}{\textbf{\#Micropost2015}} & \multicolumn{3}{c}{\textbf{\#Micropost2016}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
& Num Entities & Linkable Entities & NIL Entities & Num Entities & Linkable Entities & NIL Entities \\
\midrule
\textbf{Training} & 4016 & 3565 & 451 & 8665 & 6374 & 2291 \\
\textbf{Dev} & 790 & 428 & 362 & 338 & 253 & 85 \\
\textbf{Test} &  3860 & 2382 & 1478 & 1022 & 738 & 284 \\
\bottomrule
\end{tabularx}

\vspace{+5pt}

\begin{tabularx}{\linewidth}{CCCC}
 & \multicolumn{3}{c}{\textbf{EVLITA NEEL-IT 2016}}  \\
\cmidrule(lr){2-4}
& Num Entities & Linkable Entities & NIL Entities \\
\midrule
\textbf{Training} & 787	& 520 &	267 \\
\textbf{Test} & 357 & 226 & 131 \\
\bottomrule
\end{tabularx}

\caption{Datasets statistics}
\label{tab:datasets}
\end{table}

\section{Performance Measures}
The performance of the NEL systems is evaluated comparing the final output result with the Ground Standard (GS), which contains all the correct links for the entities. 
NEL system are evaluated using Strong Link Match (SLM). The SLM is evaluated considering also the dereference of the KB. For example, the entity ``Obama" in DBpedia redirects to ``Barack Obama". The SLM evaluates entities that are one the redirect of the other as correct. Given the gold standard we can consider each pair \(<\!entity, link\!>\) as:
\begin{itemize}[itemsep = 0.1em]
\item True Positive (\(tp\)): if the system correctly recognize the link of the entity.
\item False Positive (\(fp\)): if the link recognized by the system is different by the one in the GS.
\item True Negative (\(tn\)): if the link is not recognized by the system and in the GS. In this case the link is ``NIL".
\item False Negative (\(fn\)): if the system recognize the entity, but the entity is not recognized by the GS. In other words the system returns ''NIL" but the GS has a link.
\end{itemize} 

Using these definitions we can calculate the following performance scores for the SLM score:
\begin{itemize}[itemsep = 0.1em]
\item Precision: represents the number of correctly labeled named entities over the total number of labeled named entities. This value is defined between 0 and 1.
\begin{equation}
precision = \frac{tp}{tp+fp}
\end{equation}

\item Recall: represents the number of correctly labeled named entities over the total number of entities in the gold standard. This value is defined between 0 and 1.
\begin{equation}
recall = \frac{tp}{tn+fn}
\end{equation}
\item F1 Score: is the harmonic mean of the precision and the recall. The F1 metric weights recall and precision equally. It is defined between 0 and 1. 
\begin{equation}
F_1 = 2 \cdot \frac{precision \cdot recall}{precision+recall}
\end{equation}
\item NIL Score: is equivalent to the recall for the NIL labeled entities.
\end{itemize}	


\section{Experimental results}
\paragraph{} In this section, we will introduce the results achieved by our system and also do a quick analysis over its weaknesses. The results are presented showing the impact of the main techniques of our pipeline on the performance measures. 


\subsection{Baseline results}
The comparison is done using a baseline result which are evaluated without any preprocessing, disambiguation, or PageRank. In Figures \ref{tab:baseline_results_2015}, \ref{tab:baseline_results_2016}, and \ref{tab:baseline_results_evalita} we can see the results for both English and Italian challenges. As we can notice, the results are promising, we achieve an average \(F_1\) \textit{Score} of 40\%. 

\begin{table}[!htbp]
\centering
\footnotesize
\setlength{\tabcolsep}{0.3em}
\begin{tabularx}{\linewidth}{CCCCC}
 & \multicolumn{4}{c}{\textbf{SLM scores for \#Micropost2015}} \\
\cmidrule(lr){2-5}
 & Precision & Recall &  F1 Score & NIL Score \\
\midrule
\textbf{Training} & 0.4604 & 0.5186 & 0.4877 & 0.7472 \\
\textbf{Dev} & 0.2265 & 0.4182 & 0.2939 & 0.8895 \\
\textbf{Test} & 0.3370 & 0.5417 & 0.4168 & 0.8748 \\
\end{tabularx}
\caption{Baseline results for \#Micropost 2015}
\label{tab:baseline_results_2015}
\end{table}

\begin{table}[!htbp]
\centering
\footnotesize
\setlength{\tabcolsep}{0.3em}
\begin{tabularx}{\linewidth}{CCCCC}
 & \multicolumn{4}{c}{\textbf{SLM scores for \#Micropost2016}} \\
\cmidrule(lr){2-5}
 & Precision & Recall &  F1 Score & NIL Score\\
\midrule
\textbf{Training} & 0.3840 & 0.5221 & 0.4425 & 0.8520 \\
\textbf{Dev} & 0.3461 & 0.4624 & 0.3959 & 0.8235 \\
\textbf{Test} & 0.2563 & 0.3550 & 0.2977 & 0.8380 \\
\end{tabularx}
\caption{Baseline results for \#Micropost 2016}
\label{tab:baseline_results_2016}
\end{table}

\begin{table}[!htbp]
\centering
\footnotesize
\setlength{\tabcolsep}{0.3em}
\begin{tabularx}{\linewidth}{CCCCC}
 & \multicolumn{4}{c}{\textbf{SLM scores for EVALITA NEEL-IT 2016}} \\
\cmidrule(lr){2-5}
 & Precision & Recall &  F1 Score & NIL Score \\
\midrule
\textbf{Training} & 0.2477 & 0.3750 & 0.2983 & 0.6779 \\
\textbf{Test} & 0.238 & 0.3761 & 0.2915 & 0.5954 \\
\end{tabularx}
\caption{Baseline results for NEEL-IT 2016}
\label{tab:baseline_results_evalita}
\end{table}

\pagebreak

\subsection{Preprocessing Only}
\paragraph{}
The reference results are promising, considering that no work on entities was done. If we reduce, even in a small amount, one of the most common problem on entities from tweets, the formatting of the entity (i.e. capitalization and words separator character), our results will certainly benefit. This is where our preprocessing module is helpful, we can notice the results in Tables \ref{tab:preprocessing_results_2015} and \ref{tab:preprocessing_results_2016} that preprocessing entities increased all the scores between 10\% and 15\%. In our case, the model is very rigid over the surface form of the input entity, for example, in the reference experiment, the entity ``football\_league" is not recognized by the model, with the preprocessing the system returns the correct link using ``DBPEDIA\_ID/Football\_League". Another example is the entity ``f1", in the reference experiment this is labeled with the wrong link ``dbpedia.org/resource/Family\_1", if properly capitalized in ``F1" the result is the correct link ``dbpedia.org/resource/Formula\_One". This is also valid for the Italian tweets (Table \ref{tab:preprocessing_results_evalita}), for example the entity ``FEDEZ", an Italian singer, is recognized only in the experiment with the preprocessor enabled. The preprocessing is not always helpful, for example the entity ``repubblicait", form Italian tweets, which is the account of an Italian newspaper called ``La Repubblica", after the preprocessing is not recognized anymore.

\begin{table}[!htbp]
\centering
\footnotesize
\setlength{\tabcolsep}{0.3em}
\begin{tabularx}{\linewidth}{CCCCC}
 & \multicolumn{4}{c}{\textbf{SLM scores for \#Micropost2015}} \\
\cmidrule(lr){2-5}
 & Precision & Recall &  F1 Score & NIL Score \\
\midrule
\textbf{Training} & 0.4604 & 0.5186 & 0.4877 & 0.7472 \\
\textbf{Dev} & 0.2265 & 0.4182 & 0.2939 & 0.8895 \\
\textbf{Test} & 0.3370 & 0.5417 & 0.4168 & 0.8748 \\
\end{tabularx}
\caption{Results for \#Micropost 2015 with preprocessing}
\label{tab:preprocessing_results_2015}
\end{table}

\begin{table}[!htbp]
\centering
\footnotesize
\setlength{\tabcolsep}{0.3em}
\begin{tabularx}{\linewidth}{CCCCC}
 & \multicolumn{4}{c}{\textbf{SLM scores for \#Micropost2016}} \\
\cmidrule(lr){2-5}
 & Precision & Recall &  F1 Score & NIL Score \\
\midrule
\textbf{Training} & 0.3840 & 0.5221 & 0.4425 & 0.8520 \\
\textbf{Dev} & 0.3461 & 0.4624 & 0.3959 & 0.8235 \\
\textbf{Test} & 0.2563 & 0.3550 & 0.2977 & 0.8380 \\
\end{tabularx}
\caption{Results for \#Micropost 2016  with preprocessing}
\label{tab:preprocessing_results_2016}
\end{table}




\begin{table}[!htbp]
\centering
\footnotesize
\setlength{\tabcolsep}{0.3em}
\begin{tabularx}{\linewidth}{CCCCC}
 & \multicolumn{4}{c}{\textbf{SLM scores for EVALITA NEEL-IT 2016}} \\
\cmidrule(lr){2-5}
 & Precision & Recall &  F1 Score & NIL Score \\
\midrule
\textbf{Training} & 0.3100 & 0.4692 & 0.3733 & 0.6479 \\
\textbf{Test} & 0.2689 & 0.4247 & 0.3293 & 0.6183 \\
\end{tabularx}
\caption{Results for NEEL-IT 2016 with preprocessing}
\label{tab:preprocessing_results_evalita}
\end{table}

\newpage

\subsection{DBpedia Type}
\paragraph{}
The next experiment configuration include the preprocessing of the entity and the use of the DBpedia type to disambiguate the candidates. As we can see in Figures \ref{tab:type_results_2015}, \ref{tab:type_results_2016}, and \ref{tab:type_results_evalita} the performance increase is not so high as for the preprocessing, mostly because there is a very small amount of difference between the type of each ``DBPEDIA\_ID/Entity" in the candidates list is very small. An example when this approach work is the entity ``Interstellar", the first match of the system without type disambiguation is ``dbpedia.org/resource/Interstellar\_travel", but if we use the type provided by the NER, we can match with the type of the correct entity ``dbpedia.org/resource/Interstellar\_(film)". So the use of this information is mostly used in case of very different entities.


\begin{table}[!htbp]
\centering
\footnotesize
\setlength{\tabcolsep}{0.3em}
\begin{tabularx}{\linewidth}{CCCCC}
 & \multicolumn{4}{c}{\textbf{SLM scores for \#Micropost2015}} \\
\cmidrule(lr){2-5}
 & Precision & Recall &  F1 Score & NIL Score \\
\midrule
\textbf{Training} & 0.5333 & 0.6008 & 0.5650 & 0.7184\\
\textbf{Dev} & 0.2860 & 0.5280 & 0.3711 & 0.8729\\
\textbf{Test} & 0.4015 & 0.6507 & 0.4966 & 0.8626\\
\end{tabularx}
\caption{Results for \#Micropost 2015 with DBPedia type}
\label{tab:type_results_2015}
\end{table}

\begin{table}[!htbp]
\centering
\footnotesize
\setlength{\tabcolsep}{0.3em}
\begin{tabularx}{\linewidth}{CCCCC}
 & \multicolumn{4}{c}{\textbf{SLM scores for \#Micropost2016}} \\
\cmidrule(lr){2-5}
 & Precision & Recall &  F1 Score & NIL Score \\
\midrule
\textbf{Training} & 0.4520 & 0.6145 & 0.5209 & 0.8358 \\
\textbf{Dev} & 0.5355 & 0.7154 & 0.6125 & 0.7764 \\
\textbf{Test} & 0.4015 & 0.6507 & 0.4966 & 0.8626 \\
\end{tabularx}
\caption{Results for \#Micropost 2016  with DBPedia type}
\label{tab:type_results_2016}
\end{table}

\begin{table}[!htbp]
\centering
\footnotesize
\setlength{\tabcolsep}{0.3em}
\begin{tabularx}{\linewidth}{CCCCC}
 & \multicolumn{4}{c}{\textbf{SLM scores for EVALITA NEEL-IT 2016}} \\
\cmidrule(lr){2-5}
 & Precision & Recall &  F1 Score & NIL Score \\
\midrule
\textbf{Training} & 0.3672 & 0.5557 & 0.4422 & 0.6479 \\
\textbf{Test} & 0.3165 & 0.5000 & 0.3876 & 0.6183 \\
\end{tabularx}
\caption{Results for NEEL-IT 2016 with DBPedia type}
\label{tab:type_results_evalita}
\end{table}


\newpage

\subsection{DBpedia Disambiguation and PageRank}
\paragraph{}
The last experiment consist in the use of the DBpedia disambiguation list and the PageRank. In the Figures \ref{tab:rank_results_2015}, \ref{tab:rank_results_2016}, and \ref{tab:rank_results_evalita} we can see that this bring an increase of about 1\% but could also reduce some performance measures.  An example of entity that helps is ``Amy\_Anderson", the PageRank helps the disambiguation between Amy Anderson the architect, the golfer, and the correct one the actor. The main problem with the PageRank is that the values we used are evaluated inside the Wikipedia, and not over the web, this means that the popularity of each page is not the same as the popularity of the entity perceived by people. Also some of the entity that now are labeled wrongly are these that have a common name and are similar in popularity, for example the entity ``Google", from the correct ``dbpedia.org/resource/Google" will be wrongly disambiguated with ``dbpedia.org/resource/Google\_Search". As we can notice from Table \ref{tab:rank_results_evalita}, Italian tweets benefit more of the disambiguation process, in fact we obtained and increase of 5\% in f-measure.  For example the entity ``Monti" is easily disambiguated as the ex Italian prime minister ``Mario Monti". 


\begin{table}[!htbp]
\centering
\footnotesize
\setlength{\tabcolsep}{0.3em}
\begin{tabularx}{\linewidth}{CCCCC}
 & \multicolumn{4}{c}{\textbf{SLM scores for \#Micropost2015}} \\
\cmidrule(lr){2-5}
 & Precision & Recall &  F1 Score & NIL Score \\
\midrule
\textbf{Training} & 0.5465 & 0.6157 & 0.5790 & 0.7184 \\
\textbf{Dev} & 0.3063 & 0.5654 & 0.3973 & 0.8729 \\
\textbf{Test} & 0.4090 & 0.6628 & 0.5059 & 0.8626 \\
\end{tabularx}
\caption{Results for \#Micropost 2015 with dismbiguation and PageRank}
\label{tab:rank_results_2015}
\end{table}

\vspace{-10pt}

\begin{table}[!htbp]
\centering
\footnotesize
\setlength{\tabcolsep}{0.3em}
\begin{tabularx}{\linewidth}{CCCCC}
 & \multicolumn{4}{c}{\textbf{SLM scores for \#Micropost2016}} \\
\cmidrule(lr){2-5}
 & Precision & Recall &  F1 Score & NIL Score \\
\midrule
\textbf{Training} & 0.4633 & 0.6299 & 0.5339 & 0.8358 \\
\textbf{Dev} & 0.5295 & 0.7075 & 0.6057 & 0.7764 \\
\textbf{Test} & 0.4422 & 0.6124 & 0.5136 & 0.8169 \\
\end{tabularx}
\caption{Results for \#Micropost 2016  with dismbiguation and PageRank}
\label{tab:rank_results_2016}
\end{table}



\begin{table}[!htbp]
\centering
\footnotesize
\setlength{\tabcolsep}{0.3em}
\begin{tabularx}{\linewidth}{CCCCC}
 & \multicolumn{4}{c}{\textbf{SLM scores for EVALITA NEEL-IT 2016}} \\
\cmidrule(lr){2-5}
 & Precision & Recall &  F1 Score & NIL Score\\
\midrule
\textbf{Training} & 0.4231 & 0.6403 & 0.5095 & 0.6479\\
\textbf{Test} & 0.3529 & 0.5575 & 0.4322 & 0.6183 \\
\end{tabularx}
\caption{Results for NEEL-IT 2016 with dismbiguation and PageRank}
\label{tab:rank_results_evalita}
\end{table}


\newpage

\section{Results Comparison}
\paragraph{}
In this section we present a comparison between our system and the currently state-of-the-art solutions. As not all system present their results for the test set, we used different datasets according to data availability. Form Tables \ref{tab:comparison_micropost_2015} and \ref{tab:comparison_micropost_2016} we notice that our system performs well compared to the top systems proposed at \#Micropost challenges. In the \#Micropost 2015 challenge our system places in third position close to the solution proposed by Acubelab in second place. In the 2016 edition, we achieve a second place, with 60\% of F1 score. The main reason our system is overtaken by KEA is that their system was hardly optimized for the challenge, in fact they obtained an increase of 40\% in f-measure compared to the not optimized version. This is a weak point if a change of domain is required as it takes a lot of effort to optimize the system. A similar problem affects the solution proposed by Ousia. It is based on an in-house dictionary for acronym expansion, and a supervised learning approach using Random Forest as classifier, all of which are expensive in terms of human effort for the annotation of the data.


For EVALITA, systems NEL only results are more difficult to find, we present a comparison with two system that participated at the challenge using two different datasets. In Table \ref{tab:comparison_evalita_2016} we notice that using the training set (labeled with ``train") our solution performs similar in terms of F1 score to FBK\_NLB's solution but they differ in terms of precision and recall. Our system is less precise, but we have a higher recall. They solution is based on a Word Sense Disambiguation module and DBpedia Spotlight. We can find the same performance difference in the sisinflab solution, in this case, the higher precision is due to the combined three different approaches used for the NEL. They use DBpedia Spotlight for span and URI detection, DBpedia lookup for URI generation given a keyword, and a word embeddings model trained over tweets with a URI generator.

\begin{table}[!htbp]
\centering
\footnotesize
\setlength{\tabcolsep}{0.3em}
\begin{tabularx}{\linewidth}{CCC}
\multicolumn{3}{c}{\textbf{\#Micropost 2015 Test set}}\\
\cmidrule(lr){1-3}
Team Name & Reference & F1 Score \\
\midrule
Ousia & \cite{yamada2015end} & 0.7620 \\
Acubelab & \cite{piccinno2014tagme} & 0.5230 \\
\textbf{UNIMIB-WE} & & 0.5059 \\
UNIBA & \cite{basile2015uniba} & 0.4640
\end{tabularx}
\caption{Comparison for \#Micropost 2015 sorted by F1 Score}
\label{tab:comparison_micropost_2015}
\end{table}

\vspace{-20pt}

\begin{table}[!htbp]
\centering
\footnotesize
\setlength{\tabcolsep}{0.3em}
\begin{tabularx}{\linewidth}{CCCCC}
\multicolumn{5}{c}{\textbf{\#Micropost 2016 Dev set}} \\
\cmidrule(lr){1-5}
Team Name & Reference & Precision & Recall & F1 Score \\
\midrule
KEA & \cite{waitelonisnamed} & 0.6670 & 0.8620 & 0.7520 \\
\textbf{UNIMIB-WE} & & 0.5295 & 0.7075 & 0.6057 \\
\mbox{MIT Lincoln Lab}& \cite{greenfield2016reverse} & 0.7990 & 0.4180 & 0.5490 \\
Kanopy4Tweets & \cite{brian2016kanopy4tweets} & 0.4910 & 0.3240 & 0.3900 \\
\end{tabularx}
\caption{Comparison for \#Micropost 2016 sorted by F1 Score}
\label{tab:comparison_micropost_2016}
\end{table}


\begin{table}[!htbp]
\centering
\footnotesize
\setlength{\tabcolsep}{0.3em}
\begin{tabularx}{\columnwidth}{CCCCC}
\multicolumn{5}{c}{\textbf{EVALITA NEEL-IT}} \\
\cmidrule(lr){1-5}
Team Name & Reference & Precision & Recall & F1 Score \\
\midrule
\mbox{FBK-NLP (train)} & \cite{minard2016fbk} & 0.5980 & 0.4540 & 0.5160 \\
\mbox{\textbf{UNIMIB-WE (train)}} & & 0.4231 & 0.6403 & 0.5095 \\
\midrule
\mbox{\textbf{UNIMIB-WE (test)}} & & 0.3529 & 0.5575 & 0.4322 \\
\mbox{sisinflab (test)} & \cite{cozzasisinflab} & 0.5770 & 0.2800 & 0.3800 \\

\end{tabularx}
\caption{Comparison for NEEL-IT 2016}
\label{tab:comparison_evalita_2016}
\end{table}




