\chapter{Introduction}
\paragraph{}
With the diffusion of Internet, the Web 2.0, and the mobile devices the amount of data generated grows exponentially. A large amount of this data is generated by users. As shown by \href{http://www.internetlivestats.com/}{www.internetlivestats.com} each day 500 million tweets are sent and 5 million blog post are written. The problem with these articles and posts is that they are not structured and this mean that we cannot use them as information without pre-processing. The field of computer science that tries to generate information from unstructured data is called Information Extraction (IE). In most cases IE concerns of processing human language text using Natural Language Processing (NLP) techniques.
\section{Information Extraction}
IE can be separated in many sub-tasks based on the data required to process. Typically the main sub-tasks are:
\begin{itemize}[itemsep = 0.1em]
\item Named Entity rEcognition and Linking (NEEL): which is composted by a Named Entity Recognition (NER) system and a Named Entity Linking (NEL) system. The NER goal is to find all the named entity contained in the text. The NEL takes the named entities and links them to the KB. A named entity is a real-world object such as persons, locations, organizations, products, etc., that can be denoted with a proper name.\\
\\
IMAGE TWEET->NER->NER->WIKIPEDIA
\item Relationship extraction: finding the relations between entities in a phrase. For example for the sentence "Elon lives in California" we can extract "PERSON located in LOCATION".

\item Terminology extraction: finding the relevant terms in a text or more generally in a corpus.
\end{itemize}

For the scope of this thesis we will focus only on the NEEL task (more precisely on the linking part).
\section{Named Entity rEcognition and Linking}
\subsection{Named Entity Recognition}
\paragraph{}
Named Entity Recognition is a critical IE task, as it identifies which terms in a text are mentions of entities in the real world.
As we see in the image below, the NER takes a text (a tweet in our case) as an input and returns the entities with the corresponding type. The NER is also a pre-requisite not only for NEL but also for other IE task, including co-reference resolution, and relation extraction. In the image below we can see an example of a NER running on a tweet. \\
\newline
EXAMPLE OF A NER I/O

\paragraph{}
As we mentioned before user content are one of the biggest source of data, specifically platforms of microblogging like Facebook and Twitter. Early experiments have showed this genre to be extremely challenging for state-of-the-art algorithms of IE~\cite{derczynski2013microblog}. For instance, NER methods typically have around 90\% of accuracy on longer texts, but only 35-50\% over microblog post like tweets. The reasons of this difficulty can be summarized as follows:

\begin{itemize}[itemsep = 0.1em]
\item Shortness of microblogs: the max length of a tweet is 140 characters and this makes them hard to interpret.
\item Capitalization of the words: in a tweet, or any other microblog post, the capitalization of words may be ignored for increasing the speed of writing. The user could also deliberately overdo them with the intent of adding more emphasis to the message. This is a problem as some words change their meaning based on the capitalization of the letters. For example the words "trump" and "Trump" in a tweet could both be refereed to the president of the USA, but for a musician the first one might refer to a musical instrument and the second one to the president.
\item Word Typos: as for the capitalization a typo could significantly change the meaning of a word. In microblogs posts the amount of typos is 2.5 times greater than in a well-formed text~\cite{derczynski2015analysis}.
\item Abbreviations: given the limit on the number of characters, users tend to use abbreviations in order to write more expressive messages in the same amount of space.
\item Emotions: the meaning of a sentence can be drastically changed by an emoji. 
\end{itemize}
\paragraph{}
To overcome these problems the researchers focused on specific NERs for microblogs posts (eg Named Entity Recognition for Twitter using Conditional Random Fields by Ritter et. al~\cite{ritter2011named}).
\subsection{Named Entity Linking}
\paragraph{}
Named Entity Linking is the task of determining the identity of entities mentioned in the text. Sometimes is also called Named Entity Disambiguation (NED), and also Named Entity Normalization (NEN). It typically requires annotating a potentially ambiguous entity mention with a link to a canonical identifier, the knowledge base, describing the entity. A popular choice for the the knowledge base is Wikipedia, in which each page is considerate a named entity \\
\\
EXAMPLE OF A NEL I/O

\paragraph{}
The problems we previously introduced for the NER are also valid for the linking problem (except for the emoticons). Also named entity mention can be highly ambiguous. For example, given the sentence "Paris is the capital of France" and the entity "Paris", the idea is to determine that "Paris" is referred to the city and not to "Paris Hilton" the American celebrity. Another NEL problem is represented by the fact that the same entity could have multiple surface forms. An example could be the named entity "USA" also refereed as "America", "US", "United State of America", and "United States". We also need to keep in mind that some words recognized by the NER might not have a corresponding description in the KB, we refer to these words as Out of Vocabulary (OOV). These problems needs to be addressed by any NEL as they are very common and could decrease the overall performance. One possible way to solve these issues is by considering the context of the named entity. This is a bit more complex for tweets and other microblog posts, so the researchers found different techniques to resolve this problem that we will discuss in the next chapter.

\section{Knowledge Base}
The knowledge bases we used for our NEL system are Wikipedia and DBPedia. They are very similar in terms of content, in fact DBPedia is generated from Wikipedia data, the main difference is that the data in DBPedia are more structured. 

\subsection{DBPedia}
Is a project that aims to generate structured information from the online encyclopedia Wikipedia. The data on DBPedia follows the linked data principles. These principles are described by Tim Berners-Lee as follows:

\begin{itemize}[itemsep = 0.1em]
\item Use URIs to name (identify) things.
\item Use HTTP URIs so that these things can be looked up (interpreted, "dereferenced").
\item Provide useful information about what a name identifies when it's looked up, using open standards such as RDF, SPARQL, etc.
\item Refer to other things using their HTTP URI-based names when publishing data on the Web.
\end{itemize}

\paragraph{}
The RDF (Resource Description Framework) is a framework for conceptual description of web resources. Its based on the concept of making statements about resources. Thes statements are structurated as triples following the \textit{subject–predicate–object} structure. The subject denotes the resource, and the predicate denotes aspects of the resource, and expresses a relationship between the subject and the object. RDF is used by DBPedia for storing the information and accessing using a SQL-like language called SPARQL.

\paragraph{}
For our requirements we used DBPedia for these dump datasets:
\begin{itemize}
\item TODO
\end{itemize} 
We also used a dump of the Wikipedia database for training our model using word embeddings techniques.
\pagebreak


\nocite{sebastianruder}
\section{Word Embeddings}
\paragraph{}
A word representation is a mathematical object associated with a word, typically a vector. Word embeddings is a term coined by Yoshua Bengio et al. and are a set of language modeling and feature learning techniques in NLP used for word representation. Word embeddings are based on the idea that contextual information alone constitutes a practical representation of linguistic items. Models implementing a word representation can use Distributional Word Representation or Distributed Representation (also referred to as Word Embeddings).

\begin{itemize}[itemsep = 0.1em]
\item Distributional Word Representation: are based on co-occurrence context and based on the Distributional hypothesis:  "linguistic items with similar distributions have similar meanings". The distributional property is usually induced from document or context or textual vicinity (eg using sliding window). Some common representation techniques include Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA). Distributional techniques are memory intensive and not as efficient (not a compact representation) as distributed representations.
\item Distributed Representation: are compact, dense and low dimensional representation, with each factor in the representation containing some distinct informative property. Distributed representations comes with a cost. They are computational intensive and the algorithm is not simple like other techniques. One common approach is using neural networks. Some examples are Collobert and Weston embeddings, and Word2Vec.
\end{itemize}

\paragraph{}
Word Embeddings have also surprisingly shown that word similarity goes beyond simple syntactic regularities. It was found that we can perform simple algebraic operation on word's vector like \(vector("King")-vector("Man")+vector("Woman")\) and the result is a vector that is close to \(vector("Queen")\).

\subsection{Word2Vec}
One of the most used and popular model for word embedding is Word2Vec. It was proposed by Tomas Mikolov and his team at Google~\cite{mikolov2013efficient}. In they work they proposed two efficient model architectures for computing continuous vector representation of words. These models use a modified version of a Feedforward Neural Net Language Model (NNLM) where the hidden layer is removed to reduce the computational cost.

\paragraph{Continuous Bag-of-Words Model (CBOW)}
This model uses \(n\) words before and after the target word \(w_t\) to predict it as shown in the figure above. This architecture is called Bag-of-Words model as the order of words in the history does not influence the projection, and continuous as it uses continuous distributed representation of the context.
\paragraph{Continuous Skip-gram Model}
Is similar to the CBOW architecture, but instead of predicting the current word, the model uses the current word as a input to a classifier and predict words within a certain range before and after the current word (see figure above). Increasing the range also increases the quality of the resulting words vector, but at cost of computational complexity. Also distant words are less related to the current one so the researchers gave less weight to distant words.
\paragraph{}
Image of CBOW \& Skip-gram