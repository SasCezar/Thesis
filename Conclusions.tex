\chapter{Conclusions and Future Work}

\section{Conclusions}
\paragraph{}
In this thesis, it was introduced a NER system based on Word Embeddings. The obtained results are very promising, considering the difficulty of NEL task over entities extracted from tweets. The results also show the potential of the Word Embedding model for a high level word representation.

\section{Future Work}
\paragraph{}
We now present some possible improvements to the proposed solution, some of them are improvements to the word embedding model others are more specific to our solution.

\subsection{Wikipedia Preprocessing}
\paragraph{}
The first element on this list is the model training part, more precisely the training corpus. Even if Wikipedia is a good source of data, it still presents some noise. We noted that some words inside the models presents some unwanted chars like quotation marks, brackets, and much more. This could bring unwanted results on similarity queries, for example the word ``NOAA"  that stands for ``National Oceanic and Atmospheric Administration" returns a list containing the following elements ``NOAA's" , ``(NOAA)",  \mbox{``NOAA\textbackslash u2019s"}. These results increase the noise in the result list when querying the model and also reduces the precision of the embedding as the same word is treated as a different one.


\subsection{Linear Combination of Scores} 
Another possible improvement would be the implementation of a linear classifier that finds the best entity using all the results of all the entities generated by the model instead of the actual cascade method. We noticed that in some cases the entity is present in the candidates list generated by the model, but it is at the end of the list. A classifier could also use other similarity measures for the final decision, increasing the overall performance

\subsection{Sense2Vec}
\paragraph{}
Another interesting solution for Word Embedding is Sense2Vec, proposed by Trask et al.~\cite{trask2015sense2vec}. It is based on the Word2Vec concept but overcome one of it's main problem, each word must encode all of it's possible meaning into a single vector. This causes some vectors to be placed into a superposition that is the average of all the possible meaning of that word. Sense2Vec solves this problem by allowing multiple vectors of the same words but with different meaning defined, for example, by the part-of-speech tag. In our case we could tag the Wikipedia model using a NER for extracting the type of each entity in the training corpus.

