\chapter{Conclusions and Future Work}


This thesis introduced a NEL system based on Word Embeddings. We addressed some of the main difficulties of the NEL systems over microposts including capitalization, shortness of the text, and typos. Considering the difficulties of the investigated environment, the obtained results are very promising, proving the potential of the Word Embedding model as a high level word representation.

As future work, we aim to perform some improvements both to the word embedding model and to the approach itself.


First, we aim to improve the model training part, more precisely the training corpus. Even if Wikipedia is a good source of data, it still presents some noise. We noted that some words inside the models presents some unwanted chars like quotation marks, brackets, and much more. This could bring unwanted results on similarity queries, for example the word ``NOAA"  that stands for ``National Oceanic and Atmospheric Administration" returns a list containing the following elements ``NOAA's" , ``(NOAA)",  \mbox{``NOAA\textbackslash u2019s"}. These results increase the noise in the result list when querying the model and also reduces the precision of the embedding as the same word is treated as a different one.


Another possible improvement would be the implementation of a linear classifier that finds the best linked resource using all the results of all the candidates generated by the model instead of the actual cascade method. We noticed that in some cases the entity is present in the candidates list generated by the model, but it is not the best ranked. A classifier could also use other similarity measures for the final decision, increasing the overall performance.


Another interesting solution for Word Embedding is Sense2Vec, proposed by Trask et al.~\cite{trask2015sense2vec}. This approach is based on the Word2Vec concept but overcome one of its main problem, each word must encode all of its possible meaning into a single vector. This causes some vectors to be placed into a superposition that is the average of all the possible meaning of that word. Sense2Vec solves this problem by allowing multiple vectors of the same words but with different meaning defined, for example, by the part-of-speech tag. In our case, we could add the type information to the Wikipedia model using a NER for extracting the type of each entity in the training corpus.

