\chapter{State of the Art}
In this chapter we will introduce a quick summary of the state-of-the-art techniques in entity linking based on the survey of Shen et al.~\cite{shen2015entity}.
\paragraph{}
The researchers of \cite{shen2015entity} divide the main task in three modules:
\begin{itemize}
\item Candidate entity generation: This module extract for each entity mention a set of candidates which may contain the correct one.
\item Candidate entity ranking: Its task is to find the most likely link for the mention in the list of the candidates.
\item NIL prediction: Some mentions could not have a link, this module checks if the best candidate from the previous module is the target for the mention. 
\end{itemize}

\section{Candidates Generation}
\paragraph{}
Formally, the candidate entity generation can be described as follows:
\[\forall m \in M \; find \; E_m\]
Where:

\begin{itemize}[noitemsep,  topsep=10pt]
\item $m$ is the entity mention
\item $M$ is the set of all mentions
\item $E_m$ is the set of candidate entities for m
\end{itemize}

TO DO - Intro to the next paragraph

\paragraph{}
We can classify these techniques in three groups:
\begin{itemize}[noitemsep,  topsep=10pt]
\item Name Dictionary based techniques
\item Search Engine based techniques
\item Machine Learning based techniques
\end{itemize}

\subsection{Name Dictionary Techniques}
\paragraph{}
The structure of Wikipedia provides a useful feature for generating candidate entities, like entity pages, redirect pages, disambiguation pages and hyperlinks in Wikipedia articles. This kind of entity linking systems uses different combination of these feature to build an offline dictionary $D$ between entity name and possible mapping entities. Then the entity is compared with the dictionary's keys, the corresponding value, if the key exist, is a list of candidates for the entity. The key entity comparison can be made using exact matching or partial matching. 

\paragraph{}
This approach is used by KEA~\cite{waitelonisnamed} where they map each token to a gazetteer compiled from the DBPedia entities labels, redirect labels, and disambiguation labels being mapped to the correct DBPedia entities. For the candidate generation KEA uses an exact matching  after normalizing the entity. They also resolve overlapping token by preferring the longer tokens over short ones.

\paragraph{}
Even if name dictionary is the main technique used by many entity linking systems it has some limitation. A dictionary structure cannot represent the semantic similarity between words. Its concept of similarity between words is the value of each key, so if a word is in the list then is similar otherwise no. This problem is partially solved in systems that uses search engines.

\subsection{Search Engine Techniques}
\paragraph{}
Some entity linking systems are trying to use the whole web information for candidate generation, they use Web search engines for retrieving the list of candidates associated to the entity.

\paragraph{}
An example of this approach is the system proposed by Han and Zhao~\cite{han2009nlpr_kbp} where they submit a query containing the entity and its context to the Google API and obtained only the Wikipedia pages which are used as candidates. A similar method is used by Dredze et al.~\cite{dredze2010entity}, in this case they used as candidate list only the Wikipedia result in the top 20 Google search results. As determinated by Lehmann et al.~\cite{lehmann2010lcc} and Monahan et al.~\cite{monahan2011cross} Google search engine is very effective in resolving mappings between entity and surface form.

\paragraph{}
A different approach is used by UniMiB~\cite{caliano2016unimib}, they built an index using Apache Lucene on the title (\textit{rdf:label}), extended abstract(\textit{dbpedia:abstract}), type (\textit{dbo:type}), PageRank, and url from DBpedia. Then for candidate extraction they query Lucene using the mention and obtain a list of the documents in which the mention is contained.

\paragraph{}
As we mentioned previously, search engines resolve partially the problem of semantic similarity, this is because they analyze pages and consider the co-occurrence of two words~\cite{bollegala2007measuring}, if two words have high co-occurrence they may have similar meaning. This is a very naive approach but it gives us the idea of how a search engine could implement semantic similarity. This problem can be solved using machine learning algorithms, in particular we will see an implementation using neural networks.

\section{Candidates Ranking}
\paragraph{}
After the candidates extraction, defined in the previous section, we need to rank the candidates. The ranking process can be defined as follows:
\[\forall e_m \in E_m \; rank(e_m)\]
Where:

\begin{itemize}[noitemsep,  topsep=10pt]
\item $e_m$ is a candidate entity for the mention m
\item $E_m$ is the set of candidate entities for m
\item $rank(e_m)$ is a function that calculates the score of the candidate
\end{itemize}

\paragraph{}
The $rank$ function can be defined in many ways, and use different features and techniques for calculating the final score so, before we analyze the different techniques, is important to understand the various types of features used.

\subsection{Features}
A feature is a measurable piece of information relevant for solving a computational problem. There are a large number of feature that are used to describe different aspects of an entity. For the candidate entity ranking task the features can be divided into context-independent feature and context-dependent features.

\paragraph{Context-independent} features rely on the surface form of the entity candidate and the entity in the knowledge, the position of the entity in the text is ignored. 

\paragraph{} A simple context-independent feature is the string comparison, this could be implemented using string similarity measures like edit distance, Dice coefficient score, and left and right Hamming distance. The similarity could also be computed by training finite-state transducers. The transducers assign a score to any string pair by summing over all alignments and scoring all contained character n-grams and then combine these score using a log-linear model~\cite{dredze2010entity}.

\paragraph{} Another context-independent feature is the entity popularity. This feature is based on the principle that a popular entity is more likely to be found in a text. To express the popularity of the candidate Dredze et al.~\cite{dredze2010entity} used features from Wikipedia's graph structure like indegree and outdegree links of a node and the page size in bytes. They also used Google's PageRank indicating the rank of the candidate entity's corresponding Wikipedia page. The popularity of an entity can be very useful for increasing the performance of a system as shows an experiment of Ji and Grishman~\cite{ji2011knowledge} that evaluated a naive candidate ranking model using only the Web popularity and achieved 71\% accuracy in the TAC-KBP2010 track.

\paragraph{} We can also consider the type (i.e. people, location, and organization) of the entity as a feature, this feature is used to compare the type of the entity mention with the type of the candidate in the knowledge base. This feature can be extracted by a Named Entity Recognizer (NER) or inferred from Wikipedia and DBPedia.

\paragraph{Context-dependent} features use the textual context where the entity mention appears and also others entity mentions which need to be linked in the same document. The context allows a more precise disambiguation of the candidates. There are various form for context representation:

\begin{itemize}[topsep=10pt]
\item \textbf{Bag of words} represents the context as a bag of words collected from the text where the entity mention appears or a window of size $n$ centered on the entity. The context can be described using a bag on the whole Wikipedia entity article, or the top-k token TF-IDF (term frequencyâ€“inverse document frequency) summary of the Wikipedia page~\cite{ratinov2011local}.  

\item \textbf{Concept vector} describe the context by extracting some key-phrases, named entity, categories, Wikipedia concepts, and related links in the page. hen compose a vector to represent the semantic context of the entity in the document. Some examples can be found in the work of Dredze and his team~\cite{dredze2010entity}
\end{itemize}

\paragraph{} A very simple usage of these context representation is to calculate the distance between the vectors.

\paragraph{} A different context-dependent feature is the coherence between the entities. Many state-of-the-art linking systems rely on the idea that a document contains coherent entities form one or few related topics~\cite{hoffart2011robust}. To measure the coherence some approaches adopt the Wikipedia Link-based Measure (WLM) described in~\cite{milne2008learning} by assuming that two Wikipedia entities are semantically related if there are many Wikipedia articles that link to both.

\subsection{Techniques}
Machine learning algorithms are increasingly present in every field of computer science. Clustering algorithms, Bayesian networks and neural networks are now the state-of-the-art approach in many fields including NLP. There are two types of machine learning algorithms, supervised and unsupervised.

\paragraph{Supervised Ranking Methods} build a model that predicts the correct class for the input data. The model is trained using a set of annotated data. The model is used to assign the correct map to the entity mention.

\paragraph{Unsupervised Ranking Methods} learn from unlabeled data how to differentiate the input data. 

\section{NIL Prediction}
\paragraph{}
Some entity mentions might not have a corresponding record in the KB, therefore we have to deal with the problem of predicting unlinkable mentions.